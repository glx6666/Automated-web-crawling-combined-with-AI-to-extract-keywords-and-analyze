import re
import logging
import time
from contextlib import contextmanager
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import pymysql
import json
from datetime import datetime
from bs4 import BeautifulSoup
import logging
import sys
import hashlib

# åˆ›å»ºæ–‡ä»¶æ—¥å¿—
file_handler = logging.FileHandler('scraper.log', encoding='utf-8')

# åˆ›å»ºæµæ—¥å¿—ï¼ˆæ ‡å‡†è¾“å‡ºï¼‰ï¼Œå¹¶æ˜¾å¼æŒ‡å®š utf-8 ç¼–ç ï¼ˆé¿å… GBK ç¼–ç æŠ¥é”™ï¼‰
stream_handler = logging.StreamHandler(stream=sys.stdout)
stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
stream_handler.setStream(open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1))



logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8',
    handlers=[logging.FileHandler('product_scraper.log'), logging.StreamHandler()]
)

SITE_KEYWORDS = {
    'us': r'Best\sSellers\sRank',
    'uk': r'Best\sSellers\sRank',
    'fr': r'Classement\s+des\s+meilleures\s+ventes\s+d\'Amazon',
    'de': r'Amazon\s+Bestseller-Rang',
    'it': r'Posizione\s+nel\s+ranking\s+Venditori\s+su\s+Amazon',
    'es': r'ClasificaciÃ³n\s+en\s+los\s+mÃ¡s\s+vendidos\s+de\s+Amazon'
}


class ProductDetailScraper:
    def __init__(self, browser, db_config, nation):
        """
        å•†å“è¯¦æƒ…çˆ¬è™«
        :param browser: å·²åˆå§‹åŒ–çš„æµè§ˆå™¨å®ä¾‹
        :param db_config: æ•°æ®åº“é…ç½®å­—å…¸
        """
        self.browser = browser  # æ–°å¢æµè§ˆå™¨å®ä¾‹å¼•ç”¨
        #self.asin = asin

        if not browser.is_ready:
            raise RuntimeError("æµè§ˆå™¨å®ä¾‹æœªå®Œæˆåˆå§‹åŒ–")
        self.driver = browser.driver
        self.main_window = self.driver.current_window_handle
        self.db_config = db_config
        self.nation = nation
        self.logger = logging.getLogger(self.__class__.__name__)

    def _log_product_data(self, data):
        """æ‰“å°è°ƒè¯•ç”¨JSONå¹¶æ·»åŠ æ—¥å¿—æ ‡è®°"""
        try:
            # æ·±æ‹·è´æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            log_data = data.copy()

            # æ·»åŠ è°ƒè¯•å…ƒæ•°æ®
            log_data["_debug"] = {
                "log_timestamp": datetime.now().isoformat(),
                "current_zip": self.browser.current_zip,  # æ­£ç¡®å¼•ç”¨æµè§ˆå™¨å®ä¾‹çš„é‚®ç¼–
                "page_url": self.driver.current_url
            }

            # è½¬æ¢ä¸å¯JSONåºåˆ—åŒ–çš„å¯¹è±¡
            for key, value in log_data.items():
                if isinstance(value, bytes):
                    log_data[key] = "<binary data>"
                elif isinstance(value, datetime):
                    log_data[key] = value.isoformat()

            # ç”Ÿæˆå¸¦ç¼©è¿›çš„JSON
            json_str = json.dumps(
                log_data,
                indent=2,
                ensure_ascii=False,
                default=lambda o: str(o)
            )

            # æ·»åŠ é†’ç›®æ ‡è®°
            debug_output = f"\n{' DEBUG JSON START ':=^80}\n" \
                           f"{json_str}\n" \
                           f"{' DEBUG JSON END ':=^80}\n"

            # åŒæ—¶è¾“å‡ºåˆ°æ—¥å¿—å’Œæ§åˆ¶å°
            self.logger.info(debug_output)
            print(debug_output)  # ç¡®ä¿æ§åˆ¶å°å¯è§

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè°ƒè¯•JSONå¤±è´¥: {str(e)}")

    def judge_asin_exist(self):
        '''
        åœ¨è¾“å…¥asinæ‰“å¼€æ–°æ ‡ç­¾é¡µä¹‹åï¼Œå¦‚æœè¿™ä¸ªasinä¸å­˜åœ¨é‚£ä¹ˆäºšé©¬é€Šä¼šè¿”å›ä¸€å¼ ç‹—å›¾ç‰‡
        '''
        dog = self._safe_get_element("xpath", "//div[@id='g']/div/a/img")
        if dog is not None:
            self.logger.error("âŒ ASINä¸å­˜åœ¨")
            return False
        else:
            self.logger.info("âœ… ASINå­˜åœ¨")
            return True

    @contextmanager
    def _managed_tab(self, url):
        """ä¸Šä¸‹æ–‡ç®¡ç†ï¼šåˆ›å»ºå’Œå…³é—­ç‹¬ç«‹æ ‡ç­¾é¡µï¼ˆç§»é™¤é‚®ç¼–ä¿®æ”¹é€»è¾‘ï¼‰"""
        original_windows = self.driver.window_handles

        # ä½¿ç”¨JavaScriptæ‰“å¼€æ–°æ ‡ç­¾é¡µæ›´å¯é 
        self.driver.execute_script("window.open('');")
        new_windows = [w for w in self.driver.window_handles if w not in original_windows]

        if not new_windows:
            self.logger.error("æ— æ³•æ‰“å¼€æ–°æ ‡ç­¾é¡µ")
            raise RuntimeError("æ— æ³•æ‰“å¼€æ–°æ ‡ç­¾é¡µ")

        new_window = new_windows[0]

        try:
            self.driver.switch_to.window(new_window)
            # ç›´æ¥ä½¿ç”¨ç°æœ‰cookieè®¿é—®é¡µé¢
            self.driver.get(url)
            WebDriverWait(self.driver, 15).until(
                lambda d: d.execute_script(
                    "return document.readyState === 'complete'")
            )
            # æ£€æµ‹éªŒè¯ç 
            if self.browser._is_captcha_required():
                logging.info("âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œå¼€å§‹å¤„ç†...")
                # result = self.solve_captcha()
                result = self.browser.solve_captcha_new()
                status_msg = "âœ… å·²é€šè¿‡éªŒè¯ç " if result else "â€¼ï¸ éªŒè¯ç å¤„ç†å¤±è´¥"
                logging.info(status_msg)
            else:
                logging.info("ïŸ¢ å½“å‰æ— éªŒè¯ç ")
            # TODO åˆ¤æ–­ASINæ˜¯å¦å­˜åœ¨
            if not self.judge_asin_exist():
                raise AttributeError("Asinä¸å­˜åœ¨")
            wait = WebDriverWait(self.driver, 15)
            #print("111---------------")
            # ç­‰å¾…äº§å“æ ‡é¢˜åŠ è½½
            title_element = wait.until(
                EC.presence_of_element_located((By.ID, "productTitle"))
            )
            #print("222---------------")
            logging.info("âœ… äº§å“æ ‡é¢˜å·²åŠ è½½")
            try:
                # å°è¯•ç­‰å¾…å¸¸è§„äº”ç‚¹æè¿°
                bullet_points = wait.until(
                    EC.presence_of_element_located((By.ID, "feature-bullets"))
                )
                logging.info("âœ… äº§å“äº”ç‚¹æè¿°å·²åŠ è½½")
            except TimeoutException:
                try:
                    # å¦‚æœå¸¸è§„äº”ç‚¹æè¿°ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ID
                    bullet_points = wait.until(
                        EC.presence_of_element_located((By.ID, "featurebullets_feature_div"))
                    )
                    logging.info("âœ… äº§å“äº”ç‚¹æè¿°å·²åŠ è½½(å¤‡é€‰ID)")
                except TimeoutException:
                    logging.warning("âš ï¸ æœªæ‰¾åˆ°äº§å“äº”ç‚¹æè¿°ï¼Œå¯èƒ½æ˜¯é¡µé¢ç»“æ„ä¸åŒæˆ–äº§å“æ— æ­¤ä¿¡æ¯")
            # æ·»åŠ åŸºç¡€éªŒè¯ï¼ˆä¸æ¶‰åŠé‚®ç¼–ä¿®æ”¹ï¼‰
            if "captchacharacters" in self.driver.page_source:
                raise RuntimeError("é¡µé¢å­˜åœ¨æœªå¤„ç†éªŒè¯ç ")
            yield
        finally:
            # æ›´å®‰å…¨çš„çª—å£å…³é—­é€»è¾‘
            if len(self.driver.window_handles) > 1:
                try:
                    self.driver.close()
                except Exception as e:
                    logging.warning(f"å…³é—­æ ‡ç­¾é¡µæ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
            self.driver.switch_to.window(self.main_window)

    def scrape_and_save(self, asin,callback_sender=None, msg_id=None,
                        redis_conn=None, heartbeat_interval=None, last_heartbeat=None):
        """
        å®Œæ•´çˆ¬å–æµç¨‹ï¼ˆæ·»åŠ è°ƒè¯•è¾“å‡ºå’Œå¿ƒè·³æœºåˆ¶ï¼‰
        :param asin: å•†å“ASIN
        :param callback_sender: å›è°ƒå‡½æ•°å®ä¾‹
        :param msg_id: ä»»åŠ¡æ¶ˆæ¯ID
        :param redis_conn: Redisè¿æ¥å®ä¾‹
        :param heartbeat_interval: å¿ƒè·³é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        :param last_heartbeat: ä¸Šæ¬¡å¿ƒè·³æ—¶é—´
        :return: æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯æˆ–æˆåŠŸä¿¡æ¯
        """
        scrape_situation = None

        try:
            logging.info(f"ï¿½ï¿½ å°è¯•è®¿é—®ASINè¯¦æƒ…é¡µï¼š {asin}")
            if self.nation == 'us':
                state = 'com'
            elif self.nation == 'ca':
                state = 'ca'
            elif self.nation == 'uk':
                state = 'co.uk'
            elif self.nation == 'fr':
                state = 'fr'
            elif self.nation == 'de':
                state = 'de'
            elif self.nation == 'it':
                state = 'it'
            elif self.nation == 'es':
                state = 'es'
            with self._managed_tab(f"https://www.amazon.{state}/dp/{asin}"):
                # time.sleep(2)
                # åˆ›å»ºä¸€ä¸ªç­‰å¾…å¯¹è±¡ï¼Œæœ€å¤§ç­‰å¾…æ—¶é—´ä¸º10ç§’
                # # TODO åœ¨æ‰“å¼€é¡µé¢åï¼Œæ˜¯å¦è¦ç­‰å¾…å¿…è¦å…ƒç´ ï¼Œéœ€è¦å†è€ƒè™‘ï¼Œä¸Šçº¿å‰
                # wait = WebDriverWait(self.driver, 10)
                # # # ç­‰å¾…äº§å“æ ‡é¢˜åŠ è½½
                # title_element = wait.until(
                #     EC.presence_of_element_located((By.ID, "productTitle"))
                # )
                # logging.info("âœ… äº§å“æ ‡é¢˜å·²åŠ è½½")
                # try:
                #     # å°è¯•ç­‰å¾…å¸¸è§„äº”ç‚¹æè¿°
                #     bullet_points = wait.until(
                #         EC.presence_of_element_located((By.ID, "feature-bullets"))
                #     )
                #     logging.info("âœ… äº§å“äº”ç‚¹æè¿°å·²åŠ è½½")
                # except TimeoutException:
                #     try:
                #         # å¦‚æœå¸¸è§„äº”ç‚¹æè¿°ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„ID
                #         bullet_points = wait.until(
                #             EC.presence_of_element_located((By.ID, "featurebullets_feature_div"))
                #         )
                #         logging.info("âœ… äº§å“äº”ç‚¹æè¿°å·²åŠ è½½(å¤‡é€‰ID)")
                #     except TimeoutException:
                #         logging.warning("âš ï¸ æœªæ‰¾åˆ°äº§å“äº”ç‚¹æè¿°ï¼Œå¯èƒ½æ˜¯é¡µé¢ç»“æ„ä¸åŒæˆ–äº§å“æ— æ­¤ä¿¡æ¯")
                # self.driver.execute_script("window.stop();")
                # try:
                #     price_element = wait.until(
                #         EC.presence_of_element_located((By.ID, "priceblock_ourprice"))
                #     ) or wait.until(
                #         EC.presence_of_element_located((By.ID, "price"))
                #     ) or wait.until(
                #         EC.presence_of_element_located((By.CLASS_NAME, "a-price"))
                #     )
                #     logging.info("âœ… ä»·æ ¼ä¿¡æ¯å·²åŠ è½½")
                # except TimeoutException:
                #     logging.warning("âš ï¸ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯ï¼Œç»§ç»­å¤„ç†")
                # æ»šåŠ¨é¡µé¢ä»¥è§¦å‘ä¸€äº›æ‡’åŠ è½½å…ƒç´ ï¼Œä½†æ— éœ€æ»šåŠ¨åˆ°åº•éƒ¨
                self.driver.execute_script("window.scrollBy(0, 800);")
                time.sleep(2)

                # åœæ­¢åŠ è½½å…¶ä»–èµ„æº
                # self.driver.execute_script("window.stop();")
                logging.info(f"ï¿½ï¿½ å¼€å§‹æŠ“å–ASINï¼š {asin}")

                # åˆå§‹åŒ–æŠ“å–æ­¥éª¤
                total_steps = 5  # å‡è®¾æœ‰5ä¸ªä¸»è¦æ­¥éª¤
                current_step = 0

                # æ­¥éª¤1: æå–äº§å“æ•°æ®
                product_data = self._extract_product_data(asin)
                current_step += 1
                # self._update_heartbeat(callback_sender, msg_id, redis_conn, heartbeat_interval, last_heartbeat,
                #                        current_step, total_steps)
                # æ­¥éª¤2: æ—¥å¿—è®°å½•äº§å“æ•°æ®
                self._log_product_data(product_data)
                current_step += 1
                # self._update_heartbeat(callback_sender, msg_id, redis_conn, heartbeat_interval, last_heartbeat,
                #                        current_step, total_steps)

                # æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®åº“
                if not self._save_to_database(product_data, asin):
                    logging.info(f"TASK_FAILED, æ•°æ®åº“å†™å…¥å¤±è´¥")
                    scrape_situation = f"æ•°æ®åº“ä¿å­˜ASIN {asin} æ•°æ®å¤±è´¥"
                    self.logger.error(f"ä¿å­˜ASIN {asin} æ•°æ®å¤±è´¥")
                    return False, scrape_situation
                current_step += 1
                # self._update_heartbeat(callback_sender, msg_id, redis_conn, heartbeat_interval, last_heartbeat,
                #                        current_step, total_steps)

                # scrape_situation = f"äº‘ç«¯æ•°æ®è·å–æˆåŠŸï¼Œè¯¦æƒ…é¡µï¼šhttps://www.amazon.com/dp/{asin}"
                scrape_situation = f"https://www.amazon.{state}/dp/{asin}"
                return True, scrape_situation
        except Exception as e:
            logging.info(f"çˆ¬è™«è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
            scrape_situation = f"çˆ¬è™«è¿è¡Œæ—¶é”™è¯¯: {str(e)}"
            if 'Asinä¸å­˜åœ¨' in str(e):
                logging.info(f"Asinä¸å­˜åœ¨")
                scrape_situation = f"Asinä¸å­˜åœ¨"
                return "Asin_None", scrape_situation
            if 'HTTPConnectionPool' in str(e):
                logging.info(f"æ­¤ä»£ç†IPå‡ºç°æ•…éšœ,æ›´æ¢ä»£ç†ipè·å–æ–¹å¼")
                scrape_situation = f"æ­¤ä»£ç†IPå‡ºç°æ•…éšœ,æ›´æ¢ä»£ç†ipè·å–æ–¹å¼"
                return "HTTP", scrape_situation
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç†è¿‡æœŸå¯¼è‡´çš„é”™è¯¯
            if "ERR_PROXY_CONNECTION_FAILED" in str(e) or "ERR_TUNNEL_CONNECTION_FAILED" in str(e):
                logging.info("çˆ¬å–è¿‡ç¨‹ä¸­ä»£ç†è¿‡æœŸï¼Œåˆ·æ–°ä»£ç†åé‡è¯•")
                time.sleep(2)
                return None, scrape_situation  # # æ³¨æ„ï¼šè¿™é‡Œä¸ç›´æ¥é‡è¯•ï¼Œè€Œæ˜¯è¿”å›Noneè®©ä¸Šå±‚å‡½æ•°å¤„ç†é‡è¯•é€»è¾‘
            return False, scrape_situation

    def _safe_get_text(self, by, selector, timeout=10):
        """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬ï¼Œå…¼å®¹ text å’Œ textContent"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((by, selector))  # æ”¹æˆ presence æ›´ç¨³
            )
            text = element.text.strip()
            if not text:
                # fallback: å– textContentï¼Œæœ‰æ—¶ text æ˜¯ç©ºçš„
                text = element.get_attribute("textContent").strip()
            return text
        except:
            return ""

    def _safe_get_element(self, by, selector, timeout=10):
        '''å®‰å…¨è·å–å…ƒç´ '''
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((by, selector))
            )
            return element
        except:
            return None

    def _safe_get_elementslist(self, by, selector, timeout=10):
        '''å®‰å…¨è·å–å…ƒç´ åˆ—è¡¨'''
        try:
            # ç­‰å¾…è‡³å°‘ä¸€ä¸ªå…ƒç´ å­˜åœ¨
            elements = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_all_elements_located((by, selector))
            )
            return elements
        except:
            return []

    def hash_mod3(self, asin):
        """å¯¹asinå“ˆå¸Œå–æ¨¡åˆ†è¡¨"""
        hash_hex = hashlib.md5(asin.encode('utf-8')).hexdigest()
        # è½¬ä¸ºåè¿›åˆ¶æ•´æ•°
        hash_int = int(hash_hex, 16)
        # å¯¹ 3 å–æ¨¡
        return hash_int % 5

    def _save_to_database(self, data, asin):
        """å¸¦æ—¶é—´åˆ¤æ–­çš„ä¿å­˜é€»è¾‘"""
        try:
            with pymysql.connect(**self.db_config) as conn:
                with conn.cursor() as cursor:
                    # å‚æ•°æ ¡éªŒ
                    if not isinstance(data, dict):
                        raise ValueError("æ•°æ®å¿…é¡»ä¸ºå­—å…¸æ ¼å¼")

                    # æ£€æŸ¥å¿…è¦å­—æ®µ
                    required_fields = ['asin', 'title']
                    if not all(data.get(field) for field in required_fields):
                        self.logger.error("ç¼ºå°‘å¿…è¦å­—æ®µASINæˆ–Title")
                        return False

                    table_num = self.hash_mod3(asin)

                    # æŸ¥è¯¢æœ€æ–°è®°å½•ï¼ˆä½¿ç”¨å­—å…¸æ¸¸æ ‡ï¼‰
                    cursor.execute(f"""
                        SELECT id, created_at 
                        FROM uatu_asin_info_{self.nation}_{table_num}
                        WHERE asin = %s 
                        ORDER BY created_at DESC 
                        LIMIT 1
                    """, (data['asin'],))
                    existing_record = cursor.fetchone()

                    # å°†å…ƒç»„è½¬æ¢ä¸ºå­—å…¸
                    if existing_record:
                        column_names = [desc[0] for desc in cursor.description]
                        existing_record = dict(zip(column_names, existing_record))

                    # æ—¶é—´åˆ¤æ–­é€»è¾‘
                    should_update = False
                    if existing_record:
                        cursor.execute(f"""
                            SELECT TIMESTAMPDIFF(HOUR, created_at, NOW()) AS hours_diff 
                            FROM uatu_asin_info_{self.nation}_{table_num}
                            WHERE id = %s
                        """, (existing_record['id'],))  # ä½¿ç”¨å­—å…¸é”®è®¿é—®
                        result = cursor.fetchone()
                        if result:
                            column_names = [desc[0] for desc in cursor.description]
                            result = dict(zip(column_names, result))
                            time_diff = result['hours_diff']  # ä½¿ç”¨å­—å…¸é”®è®¿é—®
                            should_update = time_diff < 24

                    # æ•°æ®é¢„å¤„ç†
                    processed = {
                        'asin': data['asin'],
                        'title': data['title'][:1000],
                        'price': float(data['price']) if data.get('price') else None,
                        'rating': round(float(data['rating']), 2) if data.get('rating') else None,
                        'review_count': int(data['review_count']) if data.get('review_count') else 0,
                        'src': str(data['src']) if data.get('src') else None,
                        'category_name': data.get('category_name')[:255] if data.get('category_name') else None,
                        'category_rank': (data['category_rank']) if data.get('category_rank') else None,
                        'subcategory_name': data.get('subcategory_name')[:255] if data.get(
                            'subcategory_name') else None,
                        'subcategory_rank': (data['subcategory_rank']) if data.get('subcategory_rank') else None,
                        'past_month_sales': int(data['past_month_sales']) if data.get('past_month_sales') else None,
                        'about_this_item': data.get('about_this_item')[:65535] if data.get('about_this_item') else None,
                        'product_details': data.get('product_details')[:65535] if data.get('product_details') else None,
                        'customers_say': data.get('customers_say')[:65535] if data.get('customers_say') else None,
                        'select_to_learn_more': data.get('select_to_learn_more')[:65535] if data.get(
                            'select_to_learn_more') else None
                    }

                    # åŠ¨æ€ç”ŸæˆSQL
                    if should_update:
                        # ä½¿ç”¨å­—å…¸é”®è®¿é—®existing_record
                        update_sql = f"UPDATE uatu_asin_info_{self.nation}_{table_num} SET " \
                                     "title = %s, price = %s, rating = %s, review_count = %s, src = %s, " \
                                     "category_name = %s, category_rank = %s, subcategory_name = %s, " \
                                     "subcategory_rank = %s, past_month_sales = %s, " \
                                     "about_this_item = %s, product_details = %s, " \
                                     "customers_say = %s, select_to_learn_more = %s, " \
                                     "updated_at = NOW() " \
                                     "WHERE id = %s"
                        cursor.execute(update_sql, (
                            processed['title'], processed['price'], processed['rating'], processed['review_count'],
                            processed['src'],
                            processed['category_name'], processed['category_rank'], processed['subcategory_name'],
                            processed['subcategory_rank'], processed['past_month_sales'],
                            processed['about_this_item'], processed['product_details'],
                            processed['customers_say'], processed['select_to_learn_more'],
                            existing_record['id']
                        ))
                    else:
                        insert_sql = f"INSERT INTO uatu_asin_info_{self.nation}_{table_num} (" \
                                     "asin, title, price, rating, review_count, src, " \
                                     "category_name, category_rank, subcategory_name, " \
                                     "subcategory_rank, past_month_sales, " \
                                     "about_this_item, product_details, " \
                                     "customers_say, select_to_learn_more, " \
                                     "created_at, updated_at) " \
                                     "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())"
                        cursor.execute(insert_sql, (
                            processed['asin'], processed['title'], processed['price'], processed['rating'],
                            processed['review_count'], processed['src'],
                            processed['category_name'], processed['category_rank'], processed['subcategory_name'],
                            processed['subcategory_rank'], processed['past_month_sales'],
                            processed['about_this_item'], processed['product_details'],
                            processed['customers_say'], processed['select_to_learn_more']
                        ))

                    conn.commit()
                    logging.info(f"âœ… ASINå­˜å…¥æ•°æ®åº“æˆåŠŸ :uatu_asin_info_{self.nation}_{table_num} ")

                    return True

        except pymysql.err.ProgrammingError as e:
            self.logger.error(f"SQLè¯­æ³•é”™è¯¯: {e.args[0]}\n{e.args[1]}")
            conn.rollback()
            return False
        except KeyError as e:
            self.logger.error(f"å­—æ®µè®¿é—®é”™è¯¯: ä¸å­˜åœ¨å­—æ®µ {str(e)}")
            return False
        except Exception as e:
            self.logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
            return False
        except pymysql.Error as e:
            self.logger.error(f"æ•°æ®åº“é”™è¯¯: {e.args[0]} {e.args[1]}")
            conn.rollback()
            return False
        except Exception as e:
            self.logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
            return False

    def _extract_product_data(self, asin):
        """æ ¸å¿ƒæ•°æ®æå–æ–¹æ³•"""
        data = {"asin": asin}

        try:
            # åŸºç¡€ä¿¡æ¯
            data["title"] = self._safe_get_text(By.ID, "productTitle")
            if not data["title"]:
                raise ValueError("æ— æ³•æå–äº§å“æ ‡é¢˜")

            data["price"] = self._parse_price()
            #print('price',data["price"])
            data["rating"] = self._parse_rating()
            data["review_count"] = self._parse_review_count()
            data['src'] = self._src_url()

            # ç±»ç›®ä¿¡æ¯
            rank_data = self._parse_category_rank()
            #print(rank_data)
            data['category_name'] = rank_data.get("main_category")
            data['category_rank'] = rank_data.get("main_rank")
            sub_categories = rank_data.get("sub_categories")
            sub_categories_rank = ','.join(str(item['rank']) for item in sub_categories)
            sub_categories_name = ','.join(item['name'] for item in sub_categories)
            data['subcategory_rank'] = sub_categories_rank
            data['subcategory_name'] = sub_categories_name

            # é”€å”®ä¿¡æ¯
            data["past_month_sales"] = self._parse_monthly_sales()

            # è¯¦æƒ…ä¿¡æ¯
            data["about_this_item"] = self._parse_about_item()
            data["product_details"] = self._parse_technical_details()
            print('data["product_details"]',data["product_details"])
            data["customers_say"] = self._parse_customer_voices()
            data["select_to_learn_more"] = self._parse_learn_more()
            #print('dataæ•°æ®ä¸ºï¼š',data)
            return data
        except Exception as e:
            self.logger.error(f"æ•°æ®æå–å¼‚å¸¸: {str(e)}")
            raise

    def _parse_price(self):
        """è§£æä»·æ ¼ï¼Œå…¼å®¹æ‰€æœ‰å¸¸è§äºšé©¬é€Šå›½é™…ç«™"""
        try:
            # ä¼˜å…ˆä» .a-offscreen æå–
            price_str = self._safe_get_text(By.CSS_SELECTOR, ".a-offscreen")
            print("æŠ“åˆ°çš„åŸå§‹ä»·æ ¼ï¼š", price_str)
            if price_str:
                return self._convert_price(price_str)

            # é€€è€Œæ±‚å…¶æ¬¡ï¼Œä»æ•´æ•°å’Œå°æ•°éƒ¨åˆ†æ‹¼æ¥
            whole = self._safe_get_text(By.CSS_SELECTOR, ".a-price-whole")
            fraction = self._safe_get_text(By.CSS_SELECTOR, ".a-price-fraction")
            if whole and fraction:
                full_price = whole.replace(",", "").strip() + "." + fraction.strip()
                print("æŠ“åˆ°çš„åŸå§‹ä»·æ ¼ï¼š", full_price)
                return float(full_price)

            # å¤‡ç”¨ï¼šå°è¯•å…¶ä»– ID
            fallback_ids = [
                'priceblock_ourprice',
                'priceblock_dealprice',
                'tp_price_block_total_price_ww'
            ]
            for selector in fallback_ids:
                price_str = self._safe_get_text(By.ID, selector)
                if price_str:
                    return self._convert_price(price_str)

        except Exception as e:
            self.logger.error(f"è§£æä»·æ ¼æ—¶å‡ºé”™: {str(e)}")

        return None

    def _convert_price(self, price_str):
        """
        å°†åŒ…å«è´§å¸ç¬¦å·å’Œæœ¬åœ°æ ¼å¼çš„ä»·æ ¼å­—ç¬¦ä¸²ï¼ˆå¦‚ "Â£67.62", "67,62 â‚¬"ï¼‰è½¬ä¸º float
        """
        price_str = price_str.strip()

        # ç§»é™¤è´§å¸ç¬¦å·å’Œç©ºæ ¼
        price_str = re.sub(r"[^\d,\.]", "", price_str)

        # æ¬§ç³»ä»·æ ¼å¯èƒ½æ˜¯ 67,62ï¼ˆé€—å·ä¸ºå°æ•°ç‚¹ï¼‰
        if "," in price_str and price_str.count(",") == 1 and "." not in price_str:
            price_str = price_str.replace(",", ".")
        elif "," in price_str and "." in price_str:
            # ç¾å¼ï¼š1,234.56ï¼Œå»æ‰åƒåˆ†ä½é€—å·
            price_str = price_str.replace(",", "")

        try:
            return float(price_str)
        except ValueError:
            return None

    def _parse_rating(self):
        """è§£æè¯„åˆ†"""
        rating = None
        rating_str = self._safe_get_text(By.CSS_SELECTOR, "#acrPopover")
        try:
            if rating_str:
                rating_str = rating_str.replace(',', '.')
                match = re.search(r'[\d.]+', rating_str)
                if match:
                    rating = float(match.group())
        except Exception as e:
            logging.warning(f"è§£æè¯„åˆ†å‡ºé”™: {e}")
        return rating

    def _parse_review_count(self):
        """è§£æè¯„è®ºæ•°é‡"""
        count_str = self._safe_get_text(By.ID, "acrCustomerReviewText")
        if not count_str:
            count_str = self._safe_get_text(By.CSS_SELECTOR, "#reviewCountText span")
        try:
            return int(re.sub(r"\D", "", count_str))
        except:
            return 0

    def _src_url(self):
        src = self._safe_get_element(By.ID, 'landingImage')
        if src:
            src_url = src.get_attribute('src')
            return src_url
        else:
            return None

    def _parse_rank_str(self, rank_str, site="us"):
        """
        å°†æ’åå­—ç¬¦ä¸²è§£æä¸ºæ•´æ•°ï¼Œæ ¹æ®ç«™ç‚¹åˆ¤æ–­æ•°å­—æ ¼å¼ï¼š
        - ç¾å¼ï¼ˆus, ca, jpï¼‰ï¼š1,234
        - æ¬§å¼ï¼ˆuk, de, fr, it, es, nl, se, plï¼‰ï¼š1.234
        """
        if not rank_str:
            return None

        rank_str = rank_str.strip()
        european_sites = {"uk", "de", "fr", "it", "es", "nl", "se", "pl"}

        if site.lower() in european_sites:
            rank_str = rank_str.replace('.', '')
        else:
            rank_str = rank_str.replace(',', '')

        # æ¸…é™¤å…¶ä»–éæ•°å­—å­—ç¬¦
        rank_str = re.sub(r"[^\d]", "", rank_str)

        try:
            return int(rank_str)
        except ValueError:
            return None

    def _parse_category_rank(self):
        """
        è§£æäºšé©¬é€Šå•†å“ç±»ç›®æ’åï¼ŒæŒ‰ç«™ç‚¹é€‚é…å¤šè¯­è¨€
        """
        rank_data = {
            "main_category": '',
            "main_rank": '',
            "sub_categories": []
        }

        # å¤šè¯­è¨€æ­£åˆ™æ¨¡å¼é›†ä¸­å®šä¹‰
        RANK_PATTERNS_BY_SITE = {
            "us": r"#?([\d,.\s]+)\s+in\s+([^\n#()]+?)(?=\s*(?:#|\n|$))",
            "ca": r"#?([\d,.\s]+)\s+in\s+([^\n#()]+?)(?=\s*(?:#|\n|$))",
            "uk": r"#?([\d,.\s]+)\s+in\s+([^\n#()]+?)(?=\s*(?:#|\n|$))",
            "fr": r"([\dâ€¯.,]+)\s+en\s+([^\n#()]+?)(?=\s*(?:\d|\n|$))",
            "de": r"Nr\.\s*([\dâ€¯.,]+)\s+in\s+([^\n#()]+?)(?=\s*(?:Nr\.|\n|$))",
            "it": r"#?([\d,.\s]+)\s+in\s+([^\n#()]+?)(?=\s*(?:#|\n|$))",
            "es": r"#?([\d,.\s]+)\s+en\s+([^\n#()]+?)(?=\s*(?:#|\n|$))",
            # é»˜è®¤å…œåº•
            "default": r"#?([\d,.\s]+)\s+in\s+([^\n#()]+?)(?=\s*(?:#|\n|$))",
        }

        try:
            # â¬‡ï¸ å±•å¼€è¯¦æƒ…é¡µï¼ˆéƒ¨åˆ†ç«™ç‚¹ï¼‰
            try:
                expand_icon = WebDriverWait(self.driver, 5).until(
                    EC.element_to_be_clickable((
                        By.XPATH,
                        "//div[@data-csa-c-content-id='voyager-expander-btn-t2']//a[contains(@class, 'a-expander-header') and .//span[@role='heading']]"
                    ))
                )
                expand_icon.click()
                logging.info("ğŸ“‚ å±•å¼€è¯¦æƒ…é¡µæˆåŠŸ")
            except:
                logging.info("âœ… æ— éœ€å±•å¼€è¯¦æƒ…")

            # â¬‡ï¸ è·å–è¯¦æƒ…åŒºæ–‡æœ¬
            detail_section = self._safe_get_text(By.ID, "prodDetails") \
                             or self._safe_get_text(By.ID, "detailBulletsWrapper_feature_div") \
                             or self._safe_get_text(By.ID, "productDetails_detailBullets_sections1")

            if not detail_section:
                self.logger.warning("âŒ è·å–è¯¦æƒ…åŒºå¤±è´¥")
                return rank_data

            detail_section = detail_section.replace('\xa0', ' ').replace('\u202f', ' ')
            site = self.nation

            # é€‰æ‹©é€‚é…ç«™ç‚¹çš„æ­£åˆ™
            pattern = RANK_PATTERNS_BY_SITE.get(site, RANK_PATTERNS_BY_SITE["default"])
            sub_matches = re.findall(pattern, detail_section, re.IGNORECASE)

            if sub_matches:
                main_rank_str, main_cat = sub_matches[0]
                sub_categories = [
                    {"rank": self._parse_rank_str(rank.strip(), site), "name": name.strip()}
                    for rank, name in sub_matches[1:]
                ]

                rank_data = {
                    "main_rank": self._parse_rank_str(main_rank_str.strip(), site),
                    "main_category": main_cat.strip(),
                    "sub_categories": sub_categories
                }

            else:
                # ğŸ” å…œåº•åŒ¹é…
                fallback_match = re.search(r'(\d+)\s+in\s+([^\n#()]+)', detail_section)
                if fallback_match:
                    rank_data = {
                        "main_rank": self._parse_rank_str(fallback_match.group(1).strip(), site),
                        "main_category": fallback_match.group(2).strip(),
                        "sub_categories": []
                    }
                else:
                    self.logger.info("âš ï¸ æ— ç±»ç›®æ’ååŒ¹é…")

            return rank_data

        except Exception as e:
            self.logger.warning(f"ç±»ç›®è§£æå¼‚å¸¸: {str(e)}")
            return rank_data


    # def _parse_category_rank(self):
    #     """å®Œæ•´ç‰ˆç±»ç›®æ’åè§£æ"""
    #     rank_data = {
    #         "main_category": '',
    #         "main_rank": '',  # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ‹¬å·
    #         "sub_categories": []
    #     }
    #     # TODO
    #     try:
    #         expand_icon = WebDriverWait(self.driver, 5).until(
    #             EC.element_to_be_clickable((By.XPATH, "//div[@data-csa-c-content-id='voyager-expander-btn-t2']//a[contains(@class, 'a-expander-header') and .//span[@role='heading' and normalize-space()='Item details']]"))
    #         )
    #         expand_icon.click()
    #         logging.info(f"ï”„ å¸¸è§„ç‚¹å‡»ç±»ç›®ä¿¡æ¯")
    #     except:
    #         logging.info(f"ç±»ç›®ä¿¡æ¯å¯ä»¥ç›´æ¥è·å–")
    #
    #     try:
    #         if self._safe_get_text(By.ID, "prodDetails"):
    #             detail_section = self._safe_get_text(By.ID, "prodDetails")
    #         elif self._safe_get_text(By.ID, "detailBulletsWrapper_feature_div"):
    #             detail_section = self._safe_get_text(By.ID, "detailBulletsWrapper_feature_div")
    #         else:
    #             logging.error('ç±»ç›®æ’åæ— æ³•åŒ¹é…')
    #             raise
    #         # ä¿®æ­£åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆæ³¨æ„è½¬ä¹‰å¤„ç†ï¼‰
    #         main_pattern = r"""
    #             Best\sSellers\sRank
    #             .*?
    #             \#([\d,]+)\s+in\s+
    #             ([^#]+?)
    #             (
    #                 (?:\s*\#[\d,]+\s+in\s+[^#]+)+
    #             )
    #         """
    #         # å­ç±»ç›®æå–æ¨¡å¼è°ƒæ•´ï¼ˆå…è®¸æ¢è¡Œï¼‰
    #         sub_pattern = r"#([\d,]+)\s+in\s+([^\n#]+?)(?=\s*#|\s*\n|$)"
    #
    #         main_match = re.search(main_pattern, detail_section, re.DOTALL | re.IGNORECASE | re.VERBOSE)
    #         if main_match:
    #             # print("é™„ç±»ç›®åŒºå—:", main_match.group(3))  # è°ƒè¯•è¾“å‡º
    #             sub_matches = re.findall(sub_pattern, main_match.group(3))
    #
    #             rank_data = {
    #                 "main_rank": int(main_match.group(1).replace(",", "")),
    #                 "main_category": (
    #                     lambda s: re.match(r"^([^(]+)", s).group(1).strip() if re.match(r"^([^(]+)", s) else s)(
    #                     main_match.group(2).strip()),  # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ‹¬å·
    #                 "sub_categories": [
    #                     {"rank": int(r.replace(",", "")), "name": n.strip()}
    #                     for r, n in sub_matches
    #                 ]
    #             }
    #         else:
    #             for line in detail_section.split('\n'):
    #                 if "Best Sellers Rank" in line:
    #                     # åŒæ—¶æå–ä¸¤ä¸ªå€¼
    #                     match = re.search(r'#(\d+)\s+in\s+(.+)$', line)
    #                     if match:
    #                         rank_data = {
    #                             "main_rank": int(match.group(1).replace(",", "")),
    #                             "main_category": (
    #                                 lambda s: re.match(r"^([^(]+)", s).group(1).strip() if re.match(r"^([^(]+)",
    #                                                                                                 s) else s)(
    #                                 match.group(2).strip()),  # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ‹¬å·
    #                             "sub_categories": []
    #                         }
    #         return rank_data
    #
    #
    #     except Exception as e:
    #         self.logger.warning(f"ç±»ç›®è§£æå¼‚å¸¸: {str(e)}")
    #         rank_data = {
    #             "main_category": None,
    #             "main_rank": None,
    #             "sub_categories": []
    #         }
    #         return rank_data

    def _parse_monthly_sales(self):
        """è§£ææœˆé”€é‡ï¼Œå¤šç«™ç‚¹é€‚é…"""
        try:
            # æ–¹æ³•1ï¼šç­‰å¾…çˆ¶å®¹å™¨+ä½¿ç”¨.text
            detail_text = self._safe_get_text(By.ID, "socialProofingAsinFaceout_feature_div")


            #print('Parent text:', element.text)  # è‡ªåŠ¨è·å–æ‰€æœ‰å­å…ƒç´ æ–‡æœ¬
            print('detail_text:', detail_text)

            if not detail_text:
                return 0

            return self.convert_past_month_sales(detail_text, self.nation)

        except Exception as e:
            self.logger.warning(f"æœˆé”€é‡è§£æå¼‚å¸¸: {e}")
            return 0

    @staticmethod
    def convert_past_month_sales(detail_text, nation):
        """
        æå–å„ç«™ç‚¹ä¸åŒè¯­è¨€ä¸‹çš„æœˆé”€é‡æ–‡æœ¬ä¸­çš„é”€é‡æ•°å­—ã€‚
        æ”¯æŒæ ¼å¼ï¼š200+ã€2.5kã€400k+ã€4000+ã€10.000+ã€9 mil+ ç­‰ã€‚
        :param detail_text: æœˆé”€é‡ç›¸å…³æ–‡æœ¬
        :param nation: å›½å®¶/ç«™ç‚¹æ ‡è¯†ï¼ˆå¦‚ 'us', 'uk', 'fr', 'de', 'es', 'it'ï¼‰
        :return: æå–åˆ°çš„æ•´æ•°é”€é‡ï¼ˆé»˜è®¤ä¸º 0ï¼‰
        """
        detail_text = detail_text.lower().replace('\xa0', ' ').strip()
        print("è§£ææ–‡æœ¬:", detail_text)

        # é€šç”¨æå–é€»è¾‘
        def extract_number(text):
            match = re.search(r'([\d.,\s]+)\s*(k|mil)?\+?', text)
            if match:
                num_str, suffix = match.groups()
                num_str = num_str.replace(',', '').replace('.', '').replace(' ', '')
                try:
                    number = float(num_str)
                    if suffix == 'k' or suffix == 'mil':
                        number *= 1000
                    return int(number)
                except:
                    return 0
            return 0

        # å„è¯­è¨€å…³é”®è¯è¯†åˆ«ï¼ˆå¯é€‰å¢å¼ºè¿‡æ»¤ï¼‰
        keywords = {
            'us': 'bought',
            'uk': 'bought',
            'ca': 'bought',
            'fr': 'achetÃ©s',
            'de': 'gekauft',
            'es': 'comprados',
            'it': 'acquistati'
        }

        if nation in keywords:
            if keywords[nation] in detail_text:
                return extract_number(detail_text)

        # fallbackï¼Œæ–‡æœ¬é‡Œè™½ç„¶æœ‰æ•°å­—ï¼Œä½†å…³é”®è¯ä¸åŒ¹é…ä¹Ÿå°è¯•æå–
        return extract_number(detail_text)

    def _parse_about_item(self):
        """è§£æ'About this item'"""
        try:
            item_icon = WebDriverWait(self.driver, 5).until(
                EC.element_to_be_clickable((By.ID, "nic-po-expander-heading-text"))
            )
            # ç‚¹å‡»
            self.driver.execute_script("arguments[0].click();", item_icon)
            logging.info(f"ï”„ å¸¸è§„ç‚¹å‡»about_this_item")
        except:
            logging.info(f"about_this_item å¯ä»¥ç›´æ¥è·å–")
        try:
            wait = WebDriverWait(self.driver, 10)
            about_this_item = self._safe_get_text(By.ID, "feature-bullets")
            if not about_this_item:
                raise NoSuchElementException("Feature bullets element found but has no text.")
            return about_this_item
        except (NoSuchElementException, TimeoutException):
            xpaths_to_try = [
                '//*[@id="productFactsDesktopExpander"]/div[1]/ul/li/span',
                '//*[@id="productFactsDesktopExpander"]/div[1]/ul/span/li/span',
                '//*[@id="productFactsDesktopExpander"]/div[1]/ul[2]/li/span'
            ]
            for xpath in xpaths_to_try:
                try:
                    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, xpath)),
                                          message=f"Trying XPath: {xpath}")
                    about_this_item = ' '.join([elem.text.strip() for elem in elements if elem.text.strip()])
                    if about_this_item:
                        return about_this_item
                except TimeoutException:
                    continue
            return None

    def _parse_technical_details(self):
        """å…¼å®¹ table å’Œ ul ç»“æ„çš„äºšé©¬é€Šäº§å“è¯¦æƒ…è§£æ"""
        details = {}

        try:

            wait = WebDriverWait(self.driver, 15)  # æœ€é•¿ç­‰å¾…15ç§’

            # ç­‰å¾…tableæˆ–ulä»»æ„ä¸€ä¸ªç»“æ„å‡ºç°
            wait.until(
                EC.any_of(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#productDetails_detailBullets_sections1")),
                    EC.presence_of_element_located((By.CSS_SELECTOR, ".detail-bullet-list"))
                )
            )
            # å°è¯•æŸ¥æ‰¾ table ç»“æ„
            table_rows = self.driver.find_elements(By.CSS_SELECTOR, "#productDetails_detailBullets_sections1 tr")
            if table_rows:
                for row in table_rows:
                    try:
                        key_element = row.find_element(By.TAG_NAME, "th")
                        value_element = row.find_element(By.TAG_NAME, "td")

                        key = key_element.text.strip(": ").strip()
                        value = value_element.text.strip()
                        details[key] = value
                    except Exception as e:
                        print("è§£æ table è¡Œå¤±è´¥ï¼š", e)
                return str(details)

            # å¦‚æœ table æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾ ul ç»“æ„
            list_items = self.driver.find_elements(By.CSS_SELECTOR, ".detail-bullet-list li")
            if list_items:
                for item in list_items:
                    try:
                        spans = item.find_elements(By.TAG_NAME, "span")
                        if len(spans) >= 2:
                            key = spans[0].text.replace("â€â€", "").replace(":", "").strip()
                            value = spans[1].text.strip()
                            details[key] = value
                    except Exception as e:
                        print("è§£æ ul é¡¹å¤±è´¥ï¼š", e)
                return str(details)

            print("æœªæ‰¾åˆ°æ”¯æŒçš„äº§å“è¯¦æƒ…ç»“æ„ã€‚")
            return "{}"

        except Exception as e:
            print("è§£æäº§å“è¯¦æƒ…å¤±è´¥ï¼š", e)
            return "{}"

    def _parse_customer_voices(self):
        """è§£æå®¢æˆ·è¯„ä»·æ‘˜è¦"""
        try:
            wait = WebDriverWait(self.driver, 10)

            # å‘ä¸‹æ»‘åŠ¨åˆ°åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

            # è·å–é¡µé¢çš„ HTML å†…å®¹
            page_source = self.driver.page_source

            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(page_source, 'html.parser')

            # å°è¯•ä½¿ç”¨ data-hook å±æ€§æŸ¥æ‰¾å®¢æˆ·è¯„ä»·æ‘˜è¦
            summary = soup.find('div', attrs={'data-hook': 'cr-insights-widget-summary'})
            if summary:
                return summary.get_text(strip=True)

            # å¦‚æœä¸Šè¿°æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ CSS é€‰æ‹©å™¨
            customers_say = self._safe_get_text(By.CSS_SELECTOR, "p.a-spacing-small")
            return customers_say if customers_say else ""

        except Exception as e:
            self.logger.error(f"è§£æå®¢æˆ·è¯„ä»·æ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
            return ""

    def _parse_learn_more(self):
        """è§£æ'Select to learn more'"""
        try:
            wait = WebDriverWait(self.driver, 10)

            # å‘ä¸‹æ»‘åŠ¨åˆ°åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

            # è·å–é¡µé¢çš„ HTML å†…å®¹
            page_source = self.driver.page_source

            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(page_source, 'html.parser')

            # å°è¯•ä½¿ç”¨ data-hook å±æ€§æŸ¥æ‰¾é€‰æ‹©å­¦ä¹ æ›´å¤šå±æ€§
            aspects = soup.find_all(attrs={'data-hook': 'cr-insights-aspect-link'})

            # æå–æ‰€æœ‰ data-csa-c-item-id å±æ€§
            items = [a.get('data-csa-c-item-id', '') for a in aspects]

            seen = set()
            if items:
                # è¿‡æ»¤ç©ºå€¼å¹¶å»é‡
                return ','.join([i for i in items if i and (i not in seen and not seen.add(i))])
            else:
                learn_mores = self._safe_get_elementslist(By.CSS_SELECTOR, '[data-csa-c-action="infoPopOver"]')
                if learn_mores:
                    unique_texts = []
                    for element in learn_mores:
                        text = element.text.strip()
                        if text and text not in seen:
                            seen.add(text)
                            unique_texts.append(text)
                    result_unique = ','.join(unique_texts)
                    return result_unique if result_unique else ""
                else:
                    return ""


        except Exception as e:
            self.logger.error(f"è§£æ'Select to learn more'æ—¶å‡ºé”™: {str(e)}")
            return ""

    def _update_heartbeat(self, callback_sender, msg_id, redis_conn, heartbeat_interval, last_heartbeat, current_step,
                          total_steps):
        """
        æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º still_processing å¹¶å‘é€å›è°ƒ
        :param callback_sender: å›è°ƒå‡½æ•°å®ä¾‹
        :param msg_id: ä»»åŠ¡æ¶ˆæ¯ID
        :param redis_conn: Redisè¿æ¥å®ä¾‹
        :param heartbeat_interval: å¿ƒè·³é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        :param last_heartbeat: ä¸Šæ¬¡å¿ƒè·³æ—¶é—´
        :param current_step: å½“å‰æ­¥éª¤
        :param total_steps: æ€»æ­¥éª¤æ•°
        """
        now = time.time()
        if now - last_heartbeat >= heartbeat_interval:
            progress = int((current_step / total_steps) * 100)
            callback_sender("processing", progress, "ä»»åŠ¡è¿›è¡Œä¸­")
            redis_conn.hset("task_status", msg_id, "processing")
            last_heartbeat = now
        return last_heartbeat
